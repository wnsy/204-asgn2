---
title: "204-asgn2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#.libPaths("H:/R")
library(ggplot2)
library(rpart)
library(partykit)
```

#Group members
- Ayesha Wan Ismail
- Bhavisha Solanki
- Ruth Banda

# Problem setup - load data and exploratory data analysis (EDA)
We'll start by looking at the "Abalone" data. 

We are focusing on a supervised learning methods to model data. We perform EDA to find possible feature transformations and perform basic feature selection. We want to build a k-nearest neighbour regressors (a non-parametric modelling approach), and we use`lm()` function and related functions to build a linear regression model (a parametric model). 

As the abalone adds one ring to their shell each year but their first year, we could estimate the age of the abalone fairly precise by adding 1.5 to the number of rings (Credit: [Cleaning abalone data]). 



The data is supplied in a CSV file. Let's start by loading this into R:
```{r load data, echo=TRUE}
abalone <- read.csv("abalone.csv")
f <- rings~.
```


We split the data into two parts - a section for exploration and training, and another for testing.

```{r split abalone data, echo=TRUE}
train.idx <- sample(nrow(abalone), 0.5*nrow(abalone))
abalone.train <- abalone[train.idx, ]
abalone.test  <- abalone[-train.idx, ] ## note the negative indexing to REMOVE instances!
```


We should start by looking at some basic descriptive information about the data:
```{r EDA.1, echo=TRUE}
 summary(abalone.train)
```
Here, we focus on the training data instead of the entire data so that when we modify our features like doing  transformations will offer good generalisation performance.

As shown above, we are dealing with continuous data, hence the response for the number of `Rings` is our continuous data making it a regression problem. 

Then we are looking to plot the response (the measured number of rings):
```{r EDA.2, echo=TRUE}
hist(abalone.train$rings)
```
Next, we should examine the correlations in our data:
```{r EDA.3, echo=TRUE}
 cor(abalone.train[, -1])
```

```{r EDA.3.5, echo=TRUE}
plot(abalone.train, pch=19, col="orange")
```


```{r EDA.4, echo=TRUE}
abalone.train$length <- NULL
abalone.train$height <- NULL
abalone.train$weight.shucked <- NULL
abalone.train$weight.viscera <- NULL

abalone.test$length <- NULL
abalone.test$height <- NULL
abalone.test$weight.shucked <- NULL
abalone.test$weight.viscera <- NULL


```

```{r summary after cleaning data, echo=TRUE}
 summary(abalone.train)
```

Next, we should examine the correlations in our data:
```{r EDA cor after cleaning, echo=TRUE}
 cor(abalone.train[, -1])
```

```{r EDA.5, echo=TRUE}
plot(abalone.train, pch=19, col="orange")
```



get rid of length, height,weight.shucked, weight.viscera




However, we are more interested in finding whether different sexes affect the distributions. So we have to make separate plots for each sex by using `facets` argument in the `qlot()` function. It will have the tilde on the left hand side to let us know that what variable we have to separate the data by for vertically arranged panels and the right hand side lets us know what variable we want to separate by for horizontal panels. If we want only one and not both, we can use a period for the other axis, like below. The `binwidth` argument is added so `ggplot` will no longer complain (Source: [Cleaning Abalone data]).



```{r EDA.2.3 boxplot, echo=TRUE}
qplot(sex, rings,
      data = abalone,
      geom = "boxplot",
      fill = sex)
```

      
      
     
```{r EDA.2.4, echo=TRUE}
qplot(diameter, rings, 
      data = abalone, 
      color = sex)  +
  geom_smooth(slope = 1)
``` 





## Regression - k-Nearest Neighbours (kNN)

We are trying to predict the age of the abalone based on the diameter of the shell. They grow by adding new layers to their shell, which means there is an increase of the shell diameter, in addition to the entire inside of the shell which will increase the thickness of the shell (Source: [Abalone Reproduction and Growth])

We are working towards a regression problem as shown/confirmed by our EDA, so we define two functions that we will use later to evaluate our models. One is to compute mean squared error (MSE) and another is to compute the coefficient of determination (R-Squared) of predictions. These functions work on two vectors: a vector of known outcomes (`y`) and a vector of model predictions (`yhat`):


```{r regression evaluators, echo=TRUE}
MSE <- function(y, yhat) mean((y - yhat)^2)

RSQ <- function(y, yhat) 1 - sum((y - yhat)^2) / sum((y - mean(y))^2)
```


```{r knn predict, echo=TRUE}
knn <- function(f, data, k=1) {
  known.X <- scale(model.matrix(f, data)[, -1])
  known.y <- model.response(model.frame(f, data))
  
  structure(list(k=k, f=f, known.X=known.X, known.y=known.y), class="knn")
}

predict.knn <- function(knn, newdata) {
  if (class(knn) != "knn") stop("Supplied model was not a k-Nearest Neighbour object")
  
  known.X <- knn$known.X
  known.y <- knn$known.y
  
  X <- scale(model.matrix(knn$f, newdata)[, -1], 
             scale=attr(known.X, "scaled:scale"), 
             center=attr(known.X, "scaled:center"))
  
  k.nearest <- matrix(apply(X, 1, function(X.i, known.X, known.y, k) {
    d <- rowSums(sweep(known.X, 2, X.i)^2)
    known.y[head(order(d), k)]
  }, known.X=known.X, known.y=known.y, k=knn$k), nrow=knn$k)
  
  if (is.factor(known.y)) {
    factor(apply(k.nearest, 2, function(neighbours) {
      t <- table(neighbours)
      names(t[which.max(t)])
    }), levels=levels(known.y), ordered=is.ordered(known.y))
  } else if (is.character(known.y)) {
    apply(k.nearest, 2, function(neighbours) {
      t <- table(neighbours)
      names(t[which.max(t)])
    })
  } else {
    colMeans(k.nearest)
  }
}
```

apply for loop over 2 columns, 1 loop over the rows
regression take out factor and character 

```{r EDA.8, echo=TRUE}

evaluate.knn <- function(f, train, test, error, k) {
  mdl <- knn(f, train, k)
  
  ## extract the known outcomes from the test set
  y <- model.response(model.frame(f, test))
  
  ## obtain the model predictions on the test data
  yhat <- predict(mdl, test)
  
  ## returns the error of the predictions (e.g., MSE)
  error(y, yhat)
}

##honest estimation

evaluate.knn.cv <- function(f, data, K, folds, error, knn.k) {
  mean(sapply(seq(K), function(k) {
    leave.in <- data[folds != k, ] ## data not in our current fold
    hold.out <- data[folds == k, ] ## data in our current fold
    
    evaluate.knn(f, leave.in, hold.out, error, knn.k)
  }))
}

K <- 10
folds <- sample(rep(seq(K), length.out=nrow(abalone.train)))

knn.k <- c(1, 2,3	,5,	8,	13,	21,	34,	55,	89,	144,	233,	377,	610)
wse <- rep(NA, length(knn.k))
cve <- rep(NA, length(knn.k))
for (j in seq_along(knn.k)) {
  wse[j] <- evaluate.knn(f, abalone.train, abalone.train, MSE, knn.k[j])
  cve[j] <- evaluate.knn.cv(f, abalone.train, K, folds, MSE, knn.k[j])
  cat(".")
}
cat("\n")
```

LINE 169 parameters looking at knn(want something that starts off small and gets bigger and useful. samll gives nice spread and bigger doesn't)
sapply= apply ans simplify

```{r EDA.9, echo=TRUE}
plot(knn.k, wse, type="l", lwd=2, col="orange", ylab="Error", ylim=range(c(0, wse, cve)))
lines(knn.k, cve, lwd=2, col="purple")
abline(h=min(cve), lty=2, col="gray")
abline(v=knn.k[which.min(cve)], lty=2, col="gray")
text(knn.k[which.min(cve)], min(cve), sprintf("CV-min=(%d, %.2f)", knn.k[which.min(cve)], min(cve)), col="gray", adj=c(-0.05, -0.5))
legend("bottomright", c("Within-Sample", "Out-of-Sample (CV)"), col=c("orange", "purple"), lwd=2, bty="n")

```

it's a good idea to use the out-of-sample data rather than the within sample data and it gives the perfomance measure of the turning data- cross validation and test have similar charateristics.

We are trying to find the within sample and cross validation errors. 
If i give you an exam, you  k = 1, give example, give prediction in the database, so can;t use the within sample error in k because they will bias towards memorisation, false signal to, so need to have som ways of measuring, in the way it's honest, when it's applied to the data it hasn't data (cross validation: build data on the train, )


```{r , echo=TRUE}
k.selected <- knn.k[which.min(cve)]
knn.mdl <- knn(f, abalone.train, k.selected)
yhat.knn <- predict(knn.mdl, abalone.test)

```



Now we are trying to see how well the learning model performed by using the `MSE` and `RSQ` functions that we have defined above:
```{r eval eda, echo=TRUE}
print(MSE(abalone.test$rings, yhat.knn))
print(RSQ(abalone.test$rings, yhat.knn))
```


This plot will take the known response values (`y`) and the predictions of a model (`yhat`), along with some other optional formatting arguments. The function will compute the residuals of the model, and then plot the model predictions on the x-axis and the residuals on the y-axis. The plot will also add a smoothing line (called a lowess line) using the `lowess` function to smooth the relationship between `yhat` and the residuals. 

```{r fitted residuals plot, echo=TRUE}
fitted.residuals.plot <- function(y, yhat, 
                                  main=NULL, 
                                  xlim=NULL, xlab=expression(hat(y)),
                                  ylim=NULL, ylab="Residual",
                                  ...) {
  resid <- y - yhat
  
  if (is.null(ylim)) {
    r <- max(abs(range(resid)))
    ylim <- c(-r, r)
  }
  
  plot(yhat, resid, 
       xlim=xlim, xlab=xlab, 
       ylim=ylim, ylab=ylab, 
       main=main, ...)
  
  lines(lowess(yhat, resid), col="#ce2227", lty=2)
  
  abline(h=0, lty=2, col="gray")
}

```


Now we can use the previous `fitted.residuals.plot` function to visualise the `lm()` model performance that we worked on earlier, and the kNN performance:
```{r visualise, echo=TRUE}
### PUT YOUR CALLS HERE
fitted.residuals.plot(abalone.test$rings, yhat.knn)
```

Between predicted variance on the errors gets larger
errors actual magnitude of the errors....
band of errors above and below the plot that are 


 
## Linear Modelling 


```{r lm.part1 , echo=TRUE}
abalone.lm <- lm(f, abalone.train)
yhat.lm <- predict(abalone.lm, abalone.test)

```


```{r aval lm.part1 , echo=TRUE}
print(MSE(abalone.test$rings, yhat.lm))
print(RSQ(abalone.test$rings, yhat.lm))

```

```{r summary.lm , echo=TRUE}
summary(abalone.lm)

```


## (Insight)
 We also want to gain an insight of the age of the abalone based on the diameter of the shell in correlation to the number of rings (response) on the shell. This is because the abalone growth rings is due to feeding which resulted in concentric rings that are laid down at the outermost edge of the shell. They have circular patterns which can also help us predict the age of the abalones and the types of food that are available to them when they are young. 
## Tree visualisation for insight

```{r lm.part1 , echo=TRUE}

single.tree <- as.party(rpart(f, abalone.train, cp=0, xval=0, minsplit=350, minbucket = 134))
plot(single.tree)
print(single.tree)

```
not very sensitive to noise

```{r lm.part1 , echo=TRUE}


```

##
Fitting the noise, build an ensemble, you're building the forest, has signal with another set of noise, hope to all of the noise components will cancel each other out so when you average it will be 0. 


[Abalone Reproduction and Growth]: http://www.marinebio.net/marinescience/06future/abrepro.htm

[Cleaning Abalone data]: http://shapbio.me/courses/biolB215s13/abalone_cleaning.html